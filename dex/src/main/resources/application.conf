waves.dex {
  # DEX base directory
  directory = ""

  # A directory for database
  data-directory = ${waves.dex.directory}"/data"

  address-scheme-character = ""

  # An account storage
  account-storage {
    type = "not-specified" # "in-mem" or "encrypted-file"

    in-mem {
      seed-in-base64 = ""
    }

    encrypted-file {
      path = "/path/to/account.dat"
      password = "password-for-file"
    }
  }

  # NTP server
  ntp-server = "pool.ntp.org"

  # Matcher REST API settings
  rest-api {
    # Bind address
    address = "127.0.0.1"

    # Bind port
    port = 6886

    # Hash of API key string
    api-key-hash = ""

    # Enable/disable CORS support
    cors = yes

    # Enable/disable X-API-Key from different host
    api-key-different-host = no
  }

  # gRPC integration settings for DEX
  grpc.integration {

    # Address of the Waves Node DEX extension used for DEX-Node interaction via gRPC.
    # In order to provide fault-tolerance, consider using DNS server between DEX and Node, which would resolve this
    # address into several endpoints (ips of nodes with DEXExtension installed)
    waves-node-grpc {
      host = ""
      port = 6870
    }

    # Ð¡ache settings used for temporary storage of node resuqst results
    caches {
      # Default expiration time for each cache record
      default-expiration = 100ms
    }
  }

  # Base fee for the exchange transaction
  exchange-tx-base-fee = 300000

  # Settings for DEX's fee in order
  order-fee {
    # Dynamic fee in:
    #  - some asset from the predefined list or
    #  - fixed asset and fee or
    #  - percent fee in asset of the pair
    mode = "dynamic" # | "fixed" | "percent"

    # In this mode DEX charges additional fee for its
    # account script and scripts of the assets of the pair (if exists).
    # Matcher accepts fee in several assets which can be obtained by
    # the following REST request: GET /matcher/settings/rates
    # Fee is charged according to the asset rate (price of 1 Waves in that asset)
    dynamic {
      # Absolute
      base-fee = 300000
    }

    fixed {
      # Fixed fee asset
      asset = "WAVES" # | "some issued asset (base58)"
      # Minimum allowed order fee for fixed mode
      min-fee = 300000
    }

    percent {
      # Asset type for fee
      asset-type = "amount" # | "price" | "spending" | "receiving"
      # In percents
      min-fee = 0.1
    }
  }

  # Price and fee deviations (in percents).
  # If enabled, imposes the following restrictions:
  #
  #   For BUY orders:
  #     1. (1 - p) * best bid <= price <= (1 + l) * best ask
  #     2. fee >= fs * (1 - fd) * best ask * amount
  #
  #   For SELL orders:
  #     1. (1 - l) * best bid <= price <= (1 + p) * best ask
  #     2. fee >= fs * (1 - fd) * best bid * amount
  #
  # where:
  #
  #   p  = max-price-deviations.profit / 100
  #   l  = max-price-deviations.loss   / 100
  #   fd = max-price-deviations.fee    / 100
  #   fs = order-fee.percent.min-fee   / 100
  #
  #   best bid = highest price of buy
  #   best ask = lowest price of sell
  #
  # Fee restrictions (2) checks if fee is in deviation bounds, i.e. orders's fee is higher than the specified
  # percentage of fee, which client would pay for the matching with the best counter order.
  #
  # NOTE:
  #  - price restrictions (1) are applicable to any mode,
  #  - fee restrictions (2) are only applicable to the percent order fee mode (order-fee.mode = percent, see order-fee settings)
  max-price-deviations {
    # Enable/disable deviations checks
    enable = no
    # Max price deviation IN FAVOR of the client
    profit = 1000000
    # Max price deviation AGAINST the client
    loss = 1000000
    # Max fee deviation from the market price
    fee = 1000000
  }

  # Restrictions for the orders. Empty list means that there are no restrictions on the orders
  #
  # Example:
  #
  # order-restrictions = {
  #   "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS": {
  #     min-amount  = 0.001
  #     max-amount  = 1000000
  #     step-amount = 0.00000001
  #     min-price   = 0.001
  #     max-price   = 100000
  #     step-price  = 0.00000001
  #   },
  #   ...
  # }
  order-restrictions = {}

  # Matching rules' dictionary for asset pairs: pair -> rules.
  #
  # Rule:
  #
  # {
  #   start-offset = 100   # start offset to apply the rule
  #   tick-size    = 0.002 # the smallest price increment
  # }
  #
  # * Rules must be sorted in ascending order of "start-offset";
  # * A next rule should have greater "start-offset" than the previous one;
  #
  # Example:
  #
  # matching-rules = {
  #   "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS": [
  #     {
  #       start-offset = 100
  #       tick-size    = 0.002
  #     },
  #     {
  #       start-offset = 500
  #       tick-size    = 0.0025
  #     },
  #     ...
  #   ]
  # }
  matching-rules = {}

  # Postgres connection settings
  postgres {
    server-name = "localhost"
    port-number = 5435
    user = "user"
    password = "user"
    data-source-class-name = "org.postgresql.ds.PGSimpleDataSource"
  }

  # History of the orders and their events, uses Postgres
  #
  #  Defaults:
  #
  #  batch-linger-ms = 1000
  #  batch-entries   = 10000
  #
  order-history {
    # Enable/disable order history
    enabled = no
    # Time for delay between batches
    orders-batch-linger-ms = 1000
    # Etries count for the batch
    orders-batch-entries = 10000

    events-batch-linger-ms = 1000
    events-batch-entries = 10000
  }

  # Snapshots creation interval (in events)
  snapshots-interval = 1000000

  # During recovery determine the offset to start:
  # If the oldest snapshot has 2025331 offset, we start from startOldestOffset = truncate(2025331 / snapshots-interval * snapshots-interval) = 2000000.
  # This option allows to limit events from the newest snapshot also. For example, the newest snapshot was done at 3092345. startNewestOffset = 3092345 - limit-events-during-recovery
  # If this option is defined, the maximum wins = max(startOldestOffset, startNewestOffset), otherwise we start from startOldestOffset
  # limit-events-during-recovery = 2000000

  # Maximum time to recover all order books from snapshots
  snapshots-loading-timeout = 10m

  # Maximum time to recover events those observed at start
  start-events-processing-timeout = 20m

  # Maximum time to process recovered events by order books
  order-books-recovering-timeout = 10m

  # Maximum allowed amount of orders retrieved via REST
  rest-order-limit = 100

  # Base assets used as price assets
  price-assets: []

  # Blacklisted assets id
  blacklisted-assets: []

  # Blacklisted assets name
  blacklisted-names: []

  # Blacklisted addresses
  blacklisted-addresses: []

  # * yes - only "allowed-asset-pairs" are allowed to trade. Other pairs are blacklisted.
  # * no  - "allowed-asset-pairs" are permitted to trade. If a pair is not in "allowed-asset-pairs",
  #         it's checked by "blacklisted-assets" and "blacklisted-names".
  white-list-only = no

  # Example:
  # allowed-asset-pairs = [
  #  "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS"
  # ]
  allowed-asset-pairs: []

  # Set of allowed order versions
  allowed-order-versions = [1, 2]

  # Cache for /matcher/orderbook/{amountAsset}/{priceAsset}?depth=N
  order-book-snapshot-http-cache {
    # A timeout to store cache
    cache-timeout = 5s

    # Cache for these depths. When ?depth=3 is requested, returned a cache for depth of 10
    depth-ranges = [10, 100]

    # The default depth, when ?depth wasn't specified.
    # Effectively, the nearest bigger (or equal to default-depth) value will be selected from depth-ranges.
    # The maximum depth will be selectied if null specified.
    default-depth = 100
  }

  # Queue for events (order was added, order was cancelled)
  events-queue {
    # Store events locally in LevelDB
    type = "local" # Other possible values: kafka

    local {
      # If "no" - no events will be written to the queue. Useful for debugging
      enable-storing = yes

      # Interval between reads from the disk
      polling-interval = 20ms

      # Max elements per poll
      max-elements-per-poll = 100

      # Clean old records before start consuming
      clean-before-consume = yes
    }

    kafka {
      # Kafka servers in format: host1:port1,host2:port2,...
      servers = ""

      # Where events should be written and read from
      topic = "dex-events"

      # For different dex connected to the same topic should be different groups
      group = "0"

      # Consumer-related settings
      consumer {
        # The consumer is polling new messages during this time
        fetch-max-duration = 30ms

        # A maximal buffer size for polled events
        max-buffer-size = 100

        # https://kafka.apache.org/documentation/#consumerconfigs
        client {
          bootstrap.servers = ${waves.dex.events-queue.kafka.servers}
          group.id = ${waves.dex.events-queue.kafka.group}
          client.id = "consumer"

          auto.offset.reset = "earliest"
          enable.auto.commit = false

          session.timeout.ms = 10000
          max.poll.interval.ms = 11000
          max.poll.records = 100
        }
      }

      # Producer-related settings
      producer {
        # If "no" - no events will be written to the queue. Useful for debugging
        enable = yes

        # https://kafka.apache.org/documentation/#producerconfigs
        client {
          bootstrap.servers = ${waves.dex.events-queue.kafka.servers}
          client.id = "producer"

          acks = all

          request.timeout.ms = 5000
          retries = 3

          # Buffer messages into a batch for this duration
          linger.ms = 5

          # Maximum size for batch
          batch.size = 16384

          # To guarantee the order
          max.in.flight.requests.per.connection = 1

          compression.type = "none"
        }
      }
    }
  }

  # Settings for transaction broadcaster
  exchange-transaction-broadcast {
    # Broadcast exchange transactions until they are confirmed by blockchain.
    # If "no", a transaction will be broadcasted once.
    broadcast-until-confirmed = no

    # When broadcast-until-confirmed = yes
    # * Bettween checks;
    # * A transaction will not be sent more frequently than this interval.
    interval = 1 minute

    # When broadcast-until-confirmed = yes
    # Not sended transaction:
    # * Will be removed from queue after this timeout;
    # * A warning will be logged.
    max-pending-time = 15 minutes
  }
}

# WARNING: No user-configurable settings below this line.

waves.dex {
  # Timeout for REST API responses from actors.
  # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
  actor-response-timeout = ${akka.http.server.request-timeout}

  # Timeout to process consumed messages. Used in a back pressure.
  process-consumed-timeout = 10 seconds
}

akka {
  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"

    deployment {
      "/exchange-transaction-broadcast" {
        dispatcher = "akka.actor.broadcast-dispatcher"
      }
      "/addresses/history-router/*" {
        dispatcher = "akka.actor.orders-history-dispatcher"
      }
    }

    broadcast-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }

    orders-history-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }

    grpc-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 6
      throughput = 1
    }
  }

}
