# ======= DON'T FORGET TO REBUILD CONTAINERS =======

# WARNING: Deprecated settings
waves.dex {
  matcher-directory = "/opt/waves-dex"
  journal-directory = "/opt/waves-dex/journal"
  snapshots-directory = "/opt/waves-dex/snapshots"

  snapshot-store {
    class = "com.wavesplatform.dex.MatcherSnapshotStore"
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
    dir = ${waves.dex.snapshots-directory}
  }
}

waves.dex {
  # Directories
  root-directory = "/opt/waves-dex"
  data-directory = ${waves.dex.root-directory}"/data"

  address-scheme-character = ""

  # NTP server
  ntp-server = "pool.ntp.org"

  # Matcher REST API settings
  rest-api {
    # Bind address
    address = "0.0.0.0"

    # Bind port
    port = 6886

    # Hash of API key string
    api-key-hash = "7L6GpLHhA5KyJTAVc8WFHwEcyTY8fC8rRbyMCiFnM4i"

    # Enable/disable CORS support
    cors = yes

    # Enable/disable X-API-Key from different host
    api-key-different-host = no
  }

  # gRPC integration settings for DEX
  grpc.integration {

    waves-node-grpc {
      host = ""
      port = 6887
    }

    caches {
      default-expiration = 100ms
    }
  }

  # Base fee for the exchange transaction
  exchange-tx-base-fee = 300000

  # Settings for dex's fee in order
  order-fee {
    # Dynamic fee in some asset from the predefined list or
    # fixed asset and fee or
    # percent fee in asset of the pair
    mode = "dynamic" # | "fixed" | "percent"

    # In this mode dex charges additional fee for its
    # account script and scripts of the assets of the pair (if exists).
    # Matcher accepts fee in several assets which can be obtained by
    # the following REST request: GET /matcher/settings/rates
    # Fee is charged according to the asset rate (its cost in Waves)
    dynamic {
      # Absolute
      base-fee = 300000
    }

    fixed {
      # Fixed fee asset
      asset = "WAVES" # | "some issued asset (base58)"

      # Minimum allowed order fee for fixed mode
      min-fee = 300000
    }

    percent {
      # Asset type for fee
      asset-type = "amount" # | "price" | "spending" | "receiving"

      # In percents
      min-fee = 0.1
    }
  }

  # Price and fee deviations (in percents)
  max-price-deviations {
    # Enable/disable deviations checks
    enable = no
    # Max price deviation IN FAVOR of the client
    profit = 1000000
    # Max price deviation AGAINST the client
    loss = 1000000
    # Max fee deviation from the market price
    fee = 1000000
  }

  # Restrictions for the orders. Empty list means that there are no restrictions on the orders
  #
  # Example:
  #
  # order-restrictions = {
  #   "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS": {
  #     min-amount  = 0.001
  #     max-amount  = 1000000
  #     step-amount = 0.00000001
  #     min-price   = 0.001
  #     max-price   = 100000
  #     step-price  = 0.00000001
  #   },
  #   ...
  # }
  order-restrictions = {}

  # Matching rules' dictionary for asset pairs: pair -> rules.
  #
  # Rule:
  #
  # {
  #   start-offset = 100   # start offset to apply the rule
  #   tick-size    = 0.002 # the smallest price increment
  # }
  #
  # * Rules must be sorted in ascending order of "start-offset";
  # * A next rule should have greater "start-offset" than the previous one;
  #
  # Example:
  #
  # matching-rules = {
  #   "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS": [
  #     {
  #       start-offset = 100
  #       tick-size    = 0.002
  #     },
  #     {
  #       start-offset = 500
  #       tick-size    = 0.0025
  #     }
  #   ]
  # }
  matching-rules = {}

  # Postgres connection settings
  postgres {
    server-name = "localhost"
    port-number = 5435
    user = "user"
    password = "user"
    data-source-class-name = "org.postgresql.ds.PGSimpleDataSource"
  }

  # History of the orders and their events, uses Postgres
  #
  #  Defaults:
  #
  #  batch-linger-ms = 1000
  #  batch-entries   = 10000
  #
  order-history {
    # Enable/disable order history
    enabled = no

    # Time for delay between batches
    orders-batch-linger-ms = 1000
    # Etries count for the batch
    orders-batch-entries = 10000

    events-batch-linger-ms = 1000
    events-batch-entries = 10000
  }

  # Snapshots creation interval (in events)
  snapshots-interval = 1000000

  # During recovery determine the offset to start:
  # If the oldest snapshot has 2025331 offset, we start from startOldestOffset = truncate(2025331 / snapshots-interval * snapshots-interval) = 2000000.
  # This option allows to limit events from the newest snapshot also. For example, the newest snapshot was done at 3092345. startNewestOffset = 3092345 - limit-events-during-recovery
  # If this option is defined, the maximum wins = max(startOldestOffset, startNewestOffset), otherwise we start from startOldestOffset
  # limit-events-during-recovery = 2000000

  # Maximum time to recover all order books from snapshots
  snapshots-loading-timeout = 10m

  # Maximum time to recover events those observed at start
  start-events-processing-timeout = 20m

  # Maximum time to process recovered events by order books
  order-books-recovering-timeout = 10m

  # Maximum allowed amount of orders retrieved via REST
  rest-order-limit = 100

  # Base assets used as price assets
  # price-assets: []

  # Blacklisted assets id
  blacklisted-assets: []

  # Blacklisted assets name
  blacklisted-names: []

  # Blacklisted addresses
  blacklisted-addresses: []

  # * yes - only "allowed-asset-pairs" are allowed to trade. Other pairs are blacklisted.
  # * no  - "allowed-asset-pairs" are permitted to trade. If a pair is not in "allowed-asset-pairs",
  #         it's checked by "blacklisted-assets" and "blacklisted-names".
  white-list-only = no

  # Example:
  # allowed-asset-pairs = [
  #  "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS"
  # ]
  allowed-asset-pairs: []

  # Set of allowed order versions
  allowed-order-versions = [1, 2, 3]

  # Cache for /matcher/orderbook/{amountAsset}/{priceAsset}?depth=N
  order-book-snapshot-http-cache {
    # A timeout to store cache
    cache-timeout = 5s

    # Cache for these depths. When ?depth=3 is requested, returned a cache for depth of 10
    depth-ranges = [10, 100]
  }

  # Interval to buffer balance changes before process them
  balance-watching-buffer-interval = 5s

  # Queue for events (order was added, order was cancelled)
  events-queue {
    # Store events locally in LevelDB
    type = "local" # Other possible values: kafka

    local {
      # If "no" - no events will be written to the queue. Useful for debugging
      enable-storing = yes

      # Interval between reads from the disk
      polling-interval = 20ms

      # Max elements per poll
      max-elements-per-poll = 100

      # Clean old records before start consuming
      clean-before-consume = yes
    }

    kafka {
      # Kafka servers in format: host1:port1,host2:port2,...
      servers = ""

      # Where events should be written and read from
      topic = "dex-events"

      # For different dex connected to the same topic should be different groups
      group = "0"

      # Consumer-related settings
      consumer {
        # The consumer is polling new messages during this time
        fetch-max-duration = 100ms

        # A maximal buffer size for polled events
        max-buffer-size = 200

        client {
          bootstrap.servers = ${waves.dex.events-queue.kafka.servers}
          group.id = ${waves.dex.events-queue.kafka.group}
          client.id = "consumer"

          auto.offset.reset = "earliest"
          enable.auto.commit = false

          session.timeout.ms = 10000
          heartbeat.interval.ms = 5000

          default.api.timeout.ms = 10000
          request.timeout.ms = 5000

          fetch.max.bytes = 30000

          max.poll.interval.ms = 11000
          max.poll.records = 100 # Should be <= ${waves.dex.events-queue.kafka.consumer.buffer-size}
        }
      }

      # Producer-related settings
      producer {
        # If "no" - no events will be written to the queue. Useful for debugging
        enable = yes

        client {
          bootstrap.servers = ${waves.dex.events-queue.kafka.servers}
          client.id = "producer"

          acks = all

          # Buffer messages into a batch for this duration
          linger.ms = 5

          # Maximum size for batch
          batch.size = 16384

          request.timeout.ms = 5000
          delivery.timeout.ms = 6000 # > request.timeout.ms + linger.ms

          # To guarantee the order
          max.in.flight.requests.per.connection = 1

          max.request.size = 30000

          compression.type = "none"
        }
      }
    }
  }

  # Settings for transaction broadcaster
  exchange-transaction-broadcast {
    # Broadcast exchange transactions until they are confirmed by blockchain.
    # If "no", a transaction will be broadcasted once.
    broadcast-until-confirmed = no

    # When broadcast-until-confirmed = yes
    # * Bettween checks;
    # * A transaction will not be sent more frequently than this interval.
    interval = 1 minute

    # When broadcast-until-confirmed = yes
    # Not sended transaction:
    # * Will be removed from queue after this timeout;
    # * A warning will be logged.
    max-pending-time = 15 minutes
  }
}

# WARNING: No user-configurable settings below this line.

waves.dex {
  # Timeout for REST API responses from actors.
  # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
  actor-response-timeout = ${akka.http.server.request-timeout}

  # Timeout to process consumed messages. Used in a back pressure.
  process-consumed-timeout = 10 seconds
}

akka {
  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"
    serializers.dex = "com.wavesplatform.dex.model.EventSerializers"
    serialization-bindings {
      "com.wavesplatform.dex.model.Events$Event" = dex
      "com.wavesplatform.dex.market.OrderBookActor$Snapshot" = dex
      "com.wavesplatform.dex.market.MatcherActor$OrderBookCreated" = dex
      "com.wavesplatform.dex.market.MatcherActor$Snapshot" = dex
    }

    deployment {
      "/exchange-transaction-broadcast" {
        dispatcher = "akka.actor.broadcast-dispatcher"
      }
      "/addresses/history-router/*" {
        dispatcher = "akka.actor.orders-history-dispatcher"
      }
    }

    broadcast-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }

    orders-history-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }
  }

  persistence {
    journal {
      plugin = akka.persistence.journal.leveldb
      leveldb {
        dir = "/opt/waves-dex/journal"
        native = on
      }
    }
    snapshot-store.plugin = waves.dex.snapshot-store
  }
}

## TODO

waves.dex {
  address-scheme-character = "Y"

  account-storage {
    type = "in-mem"

    in-mem {
      seed-in-base64 = "jVXOEAR7WwbmTs4EMmp7jFBgkfiJv+HnzURKLMZiWqk="
    }
  }

  rest-api {
    bind-address = "0.0.0.0"
    port = 6886
  }

  events-queue {
    type = "local" # or kafka
    local.polling-interval = 100ms
    kafka.group = 0
  }

  rest-order-limit = 20
}

akka.actor.debug {
  lifecycle = on
  unhandled = on
}

akka {
  loglevel = "INFO"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters-during-shutdown = false

  http.server {
    max-connections = 128
    request-timeout = 20s
    parsing {
      max-method-length = 64
      max-content-length = 1m
    }
  }
}

include "run.conf"
include "suite.conf"
